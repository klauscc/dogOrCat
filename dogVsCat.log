I1024 23:41:18.573024 12615 caffe.cpp:217] Using GPUs 0
I1024 23:41:18.596714 12615 caffe.cpp:222] GPU 0: Tesla K40c
I1024 23:41:19.135453 12615 solver.cpp:48] Initializing solver from parameters: 
test_iter: 200
test_interval: 200
base_lr: 0.001
display: 20
max_iter: 450000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 2000
snapshot: 5000
snapshot_prefix: "/data/tmp/klaus/dogVsCat/alexnet/dogvscat_alexnet_train"
solver_mode: GPU
device_id: 0
net: "alexnet/train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
test_initialization: false
I1024 23:41:19.153100 12615 solver.cpp:91] Creating training net from net file: alexnet/train_val.prototxt
I1024 23:41:19.154373 12615 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1024 23:41:19.154464 12615 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1024 23:41:19.155009 12615 net.cpp:58] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: "train_val.txt"
    batch_size: 256
    shuffle: true
    new_height: 256
    new_width: 256
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_2"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1024 23:41:19.155424 12615 layer_factory.hpp:77] Creating layer data
I1024 23:41:19.155549 12615 net.cpp:100] Creating Layer data
I1024 23:41:19.155573 12615 net.cpp:408] data -> data
I1024 23:41:19.155691 12615 net.cpp:408] data -> label
I1024 23:41:19.156393 12615 image_data_layer.cpp:38] Opening file train_val.txt
I1024 23:41:19.164536 12615 image_data_layer.cpp:53] Shuffling data
I1024 23:41:19.166126 12615 image_data_layer.cpp:58] A total of 20000 images.
I1024 23:41:19.242346 12615 image_data_layer.cpp:85] output data size: 256,3,227,227
I1024 23:41:19.712303 12615 net.cpp:150] Setting up data
I1024 23:41:19.712370 12615 net.cpp:157] Top shape: 256 3 227 227 (39574272)
I1024 23:41:19.712378 12615 net.cpp:157] Top shape: 256 (256)
I1024 23:41:19.712383 12615 net.cpp:165] Memory required for data: 158298112
I1024 23:41:19.712400 12615 layer_factory.hpp:77] Creating layer conv1
I1024 23:41:19.712455 12615 net.cpp:100] Creating Layer conv1
I1024 23:41:19.712469 12615 net.cpp:434] conv1 <- data
I1024 23:41:19.712499 12615 net.cpp:408] conv1 -> conv1
I1024 23:41:20.113587 12615 net.cpp:150] Setting up conv1
I1024 23:41:20.113644 12615 net.cpp:157] Top shape: 256 96 55 55 (74342400)
I1024 23:41:20.113653 12615 net.cpp:165] Memory required for data: 455667712
I1024 23:41:20.113694 12615 layer_factory.hpp:77] Creating layer relu1
I1024 23:41:20.113732 12615 net.cpp:100] Creating Layer relu1
I1024 23:41:20.113744 12615 net.cpp:434] relu1 <- conv1
I1024 23:41:20.113757 12615 net.cpp:395] relu1 -> conv1 (in-place)
I1024 23:41:20.114627 12615 net.cpp:150] Setting up relu1
I1024 23:41:20.114650 12615 net.cpp:157] Top shape: 256 96 55 55 (74342400)
I1024 23:41:20.114658 12615 net.cpp:165] Memory required for data: 753037312
I1024 23:41:20.114663 12615 layer_factory.hpp:77] Creating layer norm1
I1024 23:41:20.114684 12615 net.cpp:100] Creating Layer norm1
I1024 23:41:20.114694 12615 net.cpp:434] norm1 <- conv1
I1024 23:41:20.114706 12615 net.cpp:408] norm1 -> norm1
I1024 23:41:20.115839 12615 net.cpp:150] Setting up norm1
I1024 23:41:20.115851 12615 net.cpp:157] Top shape: 256 96 55 55 (74342400)
I1024 23:41:20.115855 12615 net.cpp:165] Memory required for data: 1050406912
I1024 23:41:20.115861 12615 layer_factory.hpp:77] Creating layer pool1
I1024 23:41:20.115901 12615 net.cpp:100] Creating Layer pool1
I1024 23:41:20.115906 12615 net.cpp:434] pool1 <- norm1
I1024 23:41:20.115917 12615 net.cpp:408] pool1 -> pool1
I1024 23:41:20.115978 12615 net.cpp:150] Setting up pool1
I1024 23:41:20.115986 12615 net.cpp:157] Top shape: 256 96 27 27 (17915904)
I1024 23:41:20.115989 12615 net.cpp:165] Memory required for data: 1122070528
I1024 23:41:20.115993 12615 layer_factory.hpp:77] Creating layer conv2
I1024 23:41:20.116008 12615 net.cpp:100] Creating Layer conv2
I1024 23:41:20.116014 12615 net.cpp:434] conv2 <- pool1
I1024 23:41:20.116020 12615 net.cpp:408] conv2 -> conv2
I1024 23:41:20.136711 12615 net.cpp:150] Setting up conv2
I1024 23:41:20.136754 12615 net.cpp:157] Top shape: 256 256 27 27 (47775744)
I1024 23:41:20.136759 12615 net.cpp:165] Memory required for data: 1313173504
I1024 23:41:20.136780 12615 layer_factory.hpp:77] Creating layer relu2
I1024 23:41:20.136795 12615 net.cpp:100] Creating Layer relu2
I1024 23:41:20.136801 12615 net.cpp:434] relu2 <- conv2
I1024 23:41:20.136824 12615 net.cpp:395] relu2 -> conv2 (in-place)
I1024 23:41:20.138088 12615 net.cpp:150] Setting up relu2
I1024 23:41:20.138103 12615 net.cpp:157] Top shape: 256 256 27 27 (47775744)
I1024 23:41:20.138106 12615 net.cpp:165] Memory required for data: 1504276480
I1024 23:41:20.138110 12615 layer_factory.hpp:77] Creating layer norm2
I1024 23:41:20.138123 12615 net.cpp:100] Creating Layer norm2
I1024 23:41:20.138128 12615 net.cpp:434] norm2 <- conv2
I1024 23:41:20.138134 12615 net.cpp:408] norm2 -> norm2
I1024 23:41:20.139811 12615 net.cpp:150] Setting up norm2
I1024 23:41:20.139834 12615 net.cpp:157] Top shape: 256 256 27 27 (47775744)
I1024 23:41:20.139838 12615 net.cpp:165] Memory required for data: 1695379456
I1024 23:41:20.139843 12615 layer_factory.hpp:77] Creating layer pool2
I1024 23:41:20.139858 12615 net.cpp:100] Creating Layer pool2
I1024 23:41:20.139863 12615 net.cpp:434] pool2 <- norm2
I1024 23:41:20.139873 12615 net.cpp:408] pool2 -> pool2
I1024 23:41:20.139914 12615 net.cpp:150] Setting up pool2
I1024 23:41:20.139921 12615 net.cpp:157] Top shape: 256 256 13 13 (11075584)
I1024 23:41:20.139925 12615 net.cpp:165] Memory required for data: 1739681792
I1024 23:41:20.139931 12615 layer_factory.hpp:77] Creating layer conv3
I1024 23:41:20.139961 12615 net.cpp:100] Creating Layer conv3
I1024 23:41:20.139966 12615 net.cpp:434] conv3 <- pool2
I1024 23:41:20.139973 12615 net.cpp:408] conv3 -> conv3
I1024 23:41:20.178593 12615 net.cpp:150] Setting up conv3
I1024 23:41:20.178638 12615 net.cpp:157] Top shape: 256 384 13 13 (16613376)
I1024 23:41:20.178642 12615 net.cpp:165] Memory required for data: 1806135296
I1024 23:41:20.178660 12615 layer_factory.hpp:77] Creating layer relu3
I1024 23:41:20.178673 12615 net.cpp:100] Creating Layer relu3
I1024 23:41:20.178678 12615 net.cpp:434] relu3 <- conv3
I1024 23:41:20.178688 12615 net.cpp:395] relu3 -> conv3 (in-place)
I1024 23:41:20.180145 12615 net.cpp:150] Setting up relu3
I1024 23:41:20.180161 12615 net.cpp:157] Top shape: 256 384 13 13 (16613376)
I1024 23:41:20.180166 12615 net.cpp:165] Memory required for data: 1872588800
I1024 23:41:20.180168 12615 layer_factory.hpp:77] Creating layer conv4
I1024 23:41:20.180186 12615 net.cpp:100] Creating Layer conv4
I1024 23:41:20.180189 12615 net.cpp:434] conv4 <- conv3
I1024 23:41:20.180197 12615 net.cpp:408] conv4 -> conv4
I1024 23:41:20.211318 12615 net.cpp:150] Setting up conv4
I1024 23:41:20.211359 12615 net.cpp:157] Top shape: 256 384 13 13 (16613376)
I1024 23:41:20.211364 12615 net.cpp:165] Memory required for data: 1939042304
I1024 23:41:20.211379 12615 layer_factory.hpp:77] Creating layer relu4
I1024 23:41:20.211393 12615 net.cpp:100] Creating Layer relu4
I1024 23:41:20.211400 12615 net.cpp:434] relu4 <- conv4
I1024 23:41:20.211410 12615 net.cpp:395] relu4 -> conv4 (in-place)
I1024 23:41:20.212208 12615 net.cpp:150] Setting up relu4
I1024 23:41:20.212226 12615 net.cpp:157] Top shape: 256 384 13 13 (16613376)
I1024 23:41:20.212230 12615 net.cpp:165] Memory required for data: 2005495808
I1024 23:41:20.212234 12615 layer_factory.hpp:77] Creating layer conv5
I1024 23:41:20.212280 12615 net.cpp:100] Creating Layer conv5
I1024 23:41:20.212290 12615 net.cpp:434] conv5 <- conv4
I1024 23:41:20.212298 12615 net.cpp:408] conv5 -> conv5
I1024 23:41:20.238421 12615 net.cpp:150] Setting up conv5
I1024 23:41:20.238462 12615 net.cpp:157] Top shape: 256 256 13 13 (11075584)
I1024 23:41:20.238468 12615 net.cpp:165] Memory required for data: 2049798144
I1024 23:41:20.238492 12615 layer_factory.hpp:77] Creating layer relu5
I1024 23:41:20.238514 12615 net.cpp:100] Creating Layer relu5
I1024 23:41:20.238525 12615 net.cpp:434] relu5 <- conv5
I1024 23:41:20.238544 12615 net.cpp:395] relu5 -> conv5 (in-place)
I1024 23:41:20.239665 12615 net.cpp:150] Setting up relu5
I1024 23:41:20.239681 12615 net.cpp:157] Top shape: 256 256 13 13 (11075584)
I1024 23:41:20.239686 12615 net.cpp:165] Memory required for data: 2094100480
I1024 23:41:20.239691 12615 layer_factory.hpp:77] Creating layer pool5
I1024 23:41:20.239706 12615 net.cpp:100] Creating Layer pool5
I1024 23:41:20.239714 12615 net.cpp:434] pool5 <- conv5
I1024 23:41:20.239725 12615 net.cpp:408] pool5 -> pool5
I1024 23:41:20.239791 12615 net.cpp:150] Setting up pool5
I1024 23:41:20.239804 12615 net.cpp:157] Top shape: 256 256 6 6 (2359296)
I1024 23:41:20.239809 12615 net.cpp:165] Memory required for data: 2103537664
I1024 23:41:20.239819 12615 layer_factory.hpp:77] Creating layer fc6
I1024 23:41:20.239848 12615 net.cpp:100] Creating Layer fc6
I1024 23:41:20.239856 12615 net.cpp:434] fc6 <- pool5
I1024 23:41:20.239869 12615 net.cpp:408] fc6 -> fc6
I1024 23:41:21.606535 12615 net.cpp:150] Setting up fc6
I1024 23:41:21.606575 12615 net.cpp:157] Top shape: 256 4096 (1048576)
I1024 23:41:21.606580 12615 net.cpp:165] Memory required for data: 2107731968
I1024 23:41:21.606597 12615 layer_factory.hpp:77] Creating layer relu6
I1024 23:41:21.606617 12615 net.cpp:100] Creating Layer relu6
I1024 23:41:21.606631 12615 net.cpp:434] relu6 <- fc6
I1024 23:41:21.606645 12615 net.cpp:395] relu6 -> fc6 (in-place)
I1024 23:41:21.607139 12615 net.cpp:150] Setting up relu6
I1024 23:41:21.607152 12615 net.cpp:157] Top shape: 256 4096 (1048576)
I1024 23:41:21.607156 12615 net.cpp:165] Memory required for data: 2111926272
I1024 23:41:21.607164 12615 layer_factory.hpp:77] Creating layer drop6
I1024 23:41:21.607183 12615 net.cpp:100] Creating Layer drop6
I1024 23:41:21.607192 12615 net.cpp:434] drop6 <- fc6
I1024 23:41:21.607208 12615 net.cpp:395] drop6 -> fc6 (in-place)
I1024 23:41:21.607239 12615 net.cpp:150] Setting up drop6
I1024 23:41:21.607247 12615 net.cpp:157] Top shape: 256 4096 (1048576)
I1024 23:41:21.607250 12615 net.cpp:165] Memory required for data: 2116120576
I1024 23:41:21.607254 12615 layer_factory.hpp:77] Creating layer fc7
I1024 23:41:21.607262 12615 net.cpp:100] Creating Layer fc7
I1024 23:41:21.607265 12615 net.cpp:434] fc7 <- fc6
I1024 23:41:21.607280 12615 net.cpp:408] fc7 -> fc7
I1024 23:41:22.269529 12615 net.cpp:150] Setting up fc7
I1024 23:41:22.269590 12615 net.cpp:157] Top shape: 256 4096 (1048576)
I1024 23:41:22.269598 12615 net.cpp:165] Memory required for data: 2120314880
I1024 23:41:22.269623 12615 layer_factory.hpp:77] Creating layer relu7
I1024 23:41:22.269659 12615 net.cpp:100] Creating Layer relu7
I1024 23:41:22.269672 12615 net.cpp:434] relu7 <- fc7
I1024 23:41:22.269691 12615 net.cpp:395] relu7 -> fc7 (in-place)
I1024 23:41:22.270233 12615 net.cpp:150] Setting up relu7
I1024 23:41:22.270254 12615 net.cpp:157] Top shape: 256 4096 (1048576)
I1024 23:41:22.270261 12615 net.cpp:165] Memory required for data: 2124509184
I1024 23:41:22.270267 12615 layer_factory.hpp:77] Creating layer drop7
I1024 23:41:22.270283 12615 net.cpp:100] Creating Layer drop7
I1024 23:41:22.270293 12615 net.cpp:434] drop7 <- fc7
I1024 23:41:22.270303 12615 net.cpp:395] drop7 -> fc7 (in-place)
I1024 23:41:22.270354 12615 net.cpp:150] Setting up drop7
I1024 23:41:22.270364 12615 net.cpp:157] Top shape: 256 4096 (1048576)
I1024 23:41:22.270370 12615 net.cpp:165] Memory required for data: 2128703488
I1024 23:41:22.270377 12615 layer_factory.hpp:77] Creating layer fc8_2
I1024 23:41:22.270418 12615 net.cpp:100] Creating Layer fc8_2
I1024 23:41:22.270426 12615 net.cpp:434] fc8_2 <- fc7
I1024 23:41:22.270439 12615 net.cpp:408] fc8_2 -> fc8
I1024 23:41:22.271872 12615 net.cpp:150] Setting up fc8_2
I1024 23:41:22.271905 12615 net.cpp:157] Top shape: 256 2 (512)
I1024 23:41:22.271909 12615 net.cpp:165] Memory required for data: 2128705536
I1024 23:41:22.271920 12615 layer_factory.hpp:77] Creating layer loss
I1024 23:41:22.271934 12615 net.cpp:100] Creating Layer loss
I1024 23:41:22.271941 12615 net.cpp:434] loss <- fc8
I1024 23:41:22.271947 12615 net.cpp:434] loss <- label
I1024 23:41:22.271962 12615 net.cpp:408] loss -> loss
I1024 23:41:22.271983 12615 layer_factory.hpp:77] Creating layer loss
I1024 23:41:22.272533 12615 net.cpp:150] Setting up loss
I1024 23:41:22.272548 12615 net.cpp:157] Top shape: (1)
I1024 23:41:22.272553 12615 net.cpp:160]     with loss weight 1
I1024 23:41:22.272574 12615 net.cpp:165] Memory required for data: 2128705540
I1024 23:41:22.272578 12615 net.cpp:226] loss needs backward computation.
I1024 23:41:22.272583 12615 net.cpp:226] fc8_2 needs backward computation.
I1024 23:41:22.272586 12615 net.cpp:226] drop7 needs backward computation.
I1024 23:41:22.272589 12615 net.cpp:226] relu7 needs backward computation.
I1024 23:41:22.272593 12615 net.cpp:226] fc7 needs backward computation.
I1024 23:41:22.272596 12615 net.cpp:226] drop6 needs backward computation.
I1024 23:41:22.272599 12615 net.cpp:226] relu6 needs backward computation.
I1024 23:41:22.272603 12615 net.cpp:226] fc6 needs backward computation.
I1024 23:41:22.272608 12615 net.cpp:226] pool5 needs backward computation.
I1024 23:41:22.272610 12615 net.cpp:226] relu5 needs backward computation.
I1024 23:41:22.272615 12615 net.cpp:226] conv5 needs backward computation.
I1024 23:41:22.272620 12615 net.cpp:226] relu4 needs backward computation.
I1024 23:41:22.272626 12615 net.cpp:226] conv4 needs backward computation.
I1024 23:41:22.272630 12615 net.cpp:226] relu3 needs backward computation.
I1024 23:41:22.272634 12615 net.cpp:226] conv3 needs backward computation.
I1024 23:41:22.272642 12615 net.cpp:226] pool2 needs backward computation.
I1024 23:41:22.272649 12615 net.cpp:226] norm2 needs backward computation.
I1024 23:41:22.272653 12615 net.cpp:226] relu2 needs backward computation.
I1024 23:41:22.272657 12615 net.cpp:226] conv2 needs backward computation.
I1024 23:41:22.272663 12615 net.cpp:226] pool1 needs backward computation.
I1024 23:41:22.272668 12615 net.cpp:226] norm1 needs backward computation.
I1024 23:41:22.272672 12615 net.cpp:226] relu1 needs backward computation.
I1024 23:41:22.272675 12615 net.cpp:226] conv1 needs backward computation.
I1024 23:41:22.272680 12615 net.cpp:228] data does not need backward computation.
I1024 23:41:22.272686 12615 net.cpp:270] This network produces output loss
I1024 23:41:22.272702 12615 net.cpp:283] Network initialization done.
I1024 23:41:22.273618 12615 solver.cpp:181] Creating test net (#0) specified by net file: alexnet/train_val.prototxt
I1024 23:41:22.273677 12615 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1024 23:41:22.273879 12615 net.cpp:58] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: "test.txt"
    batch_size: 50
    shuffle: true
    new_height: 256
    new_width: 256
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_2"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1024 23:41:22.274075 12615 layer_factory.hpp:77] Creating layer data
I1024 23:41:22.274104 12615 net.cpp:100] Creating Layer data
I1024 23:41:22.274111 12615 net.cpp:408] data -> data
I1024 23:41:22.274125 12615 net.cpp:408] data -> label
I1024 23:41:22.274139 12615 image_data_layer.cpp:38] Opening file test.txt
I1024 23:41:22.275943 12615 image_data_layer.cpp:53] Shuffling data
I1024 23:41:22.276309 12615 image_data_layer.cpp:58] A total of 5000 images.
I1024 23:41:22.279281 12615 image_data_layer.cpp:85] output data size: 50,3,227,227
I1024 23:41:22.840857 12615 net.cpp:150] Setting up data
I1024 23:41:22.840973 12615 net.cpp:157] Top shape: 50 3 227 227 (7729350)
I1024 23:41:22.840984 12615 net.cpp:157] Top shape: 50 (50)
I1024 23:41:22.840989 12615 net.cpp:165] Memory required for data: 30917600
I1024 23:41:22.841001 12615 layer_factory.hpp:77] Creating layer label_data_1_split
I1024 23:41:22.841034 12615 net.cpp:100] Creating Layer label_data_1_split
I1024 23:41:22.841043 12615 net.cpp:434] label_data_1_split <- label
I1024 23:41:22.841059 12615 net.cpp:408] label_data_1_split -> label_data_1_split_0
I1024 23:41:22.841083 12615 net.cpp:408] label_data_1_split -> label_data_1_split_1
I1024 23:41:22.841220 12615 net.cpp:150] Setting up label_data_1_split
I1024 23:41:22.841234 12615 net.cpp:157] Top shape: 50 (50)
I1024 23:41:22.841241 12615 net.cpp:157] Top shape: 50 (50)
I1024 23:41:22.841249 12615 net.cpp:165] Memory required for data: 30918000
I1024 23:41:22.841255 12615 layer_factory.hpp:77] Creating layer conv1
I1024 23:41:22.841285 12615 net.cpp:100] Creating Layer conv1
I1024 23:41:22.841295 12615 net.cpp:434] conv1 <- data
I1024 23:41:22.841310 12615 net.cpp:408] conv1 -> conv1
I1024 23:41:22.868402 12615 net.cpp:150] Setting up conv1
I1024 23:41:22.868444 12615 net.cpp:157] Top shape: 50 96 55 55 (14520000)
I1024 23:41:22.868451 12615 net.cpp:165] Memory required for data: 88998000
I1024 23:41:22.868479 12615 layer_factory.hpp:77] Creating layer relu1
I1024 23:41:22.868499 12615 net.cpp:100] Creating Layer relu1
I1024 23:41:22.868505 12615 net.cpp:434] relu1 <- conv1
I1024 23:41:22.868520 12615 net.cpp:395] relu1 -> conv1 (in-place)
I1024 23:41:22.868777 12615 net.cpp:150] Setting up relu1
I1024 23:41:22.868788 12615 net.cpp:157] Top shape: 50 96 55 55 (14520000)
I1024 23:41:22.868794 12615 net.cpp:165] Memory required for data: 147078000
I1024 23:41:22.868799 12615 layer_factory.hpp:77] Creating layer norm1
I1024 23:41:22.868826 12615 net.cpp:100] Creating Layer norm1
I1024 23:41:22.868836 12615 net.cpp:434] norm1 <- conv1
I1024 23:41:22.868845 12615 net.cpp:408] norm1 -> norm1
I1024 23:41:22.870712 12615 net.cpp:150] Setting up norm1
I1024 23:41:22.870754 12615 net.cpp:157] Top shape: 50 96 55 55 (14520000)
I1024 23:41:22.870761 12615 net.cpp:165] Memory required for data: 205158000
I1024 23:41:22.870769 12615 layer_factory.hpp:77] Creating layer pool1
I1024 23:41:22.870790 12615 net.cpp:100] Creating Layer pool1
I1024 23:41:22.870798 12615 net.cpp:434] pool1 <- norm1
I1024 23:41:22.870813 12615 net.cpp:408] pool1 -> pool1
I1024 23:41:22.870884 12615 net.cpp:150] Setting up pool1
I1024 23:41:22.870899 12615 net.cpp:157] Top shape: 50 96 27 27 (3499200)
I1024 23:41:22.870904 12615 net.cpp:165] Memory required for data: 219154800
I1024 23:41:22.870909 12615 layer_factory.hpp:77] Creating layer conv2
I1024 23:41:22.870930 12615 net.cpp:100] Creating Layer conv2
I1024 23:41:22.870936 12615 net.cpp:434] conv2 <- pool1
I1024 23:41:22.870949 12615 net.cpp:408] conv2 -> conv2
I1024 23:41:22.896951 12615 net.cpp:150] Setting up conv2
I1024 23:41:22.896989 12615 net.cpp:157] Top shape: 50 256 27 27 (9331200)
I1024 23:41:22.896993 12615 net.cpp:165] Memory required for data: 256479600
I1024 23:41:22.897014 12615 layer_factory.hpp:77] Creating layer relu2
I1024 23:41:22.897032 12615 net.cpp:100] Creating Layer relu2
I1024 23:41:22.897037 12615 net.cpp:434] relu2 <- conv2
I1024 23:41:22.897076 12615 net.cpp:395] relu2 -> conv2 (in-place)
I1024 23:41:22.898382 12615 net.cpp:150] Setting up relu2
I1024 23:41:22.898422 12615 net.cpp:157] Top shape: 50 256 27 27 (9331200)
I1024 23:41:22.898428 12615 net.cpp:165] Memory required for data: 293804400
I1024 23:41:22.898437 12615 layer_factory.hpp:77] Creating layer norm2
I1024 23:41:22.898491 12615 net.cpp:100] Creating Layer norm2
I1024 23:41:22.898499 12615 net.cpp:434] norm2 <- conv2
I1024 23:41:22.898520 12615 net.cpp:408] norm2 -> norm2
I1024 23:41:22.899631 12615 net.cpp:150] Setting up norm2
I1024 23:41:22.899662 12615 net.cpp:157] Top shape: 50 256 27 27 (9331200)
I1024 23:41:22.899667 12615 net.cpp:165] Memory required for data: 331129200
I1024 23:41:22.899672 12615 layer_factory.hpp:77] Creating layer pool2
I1024 23:41:22.899703 12615 net.cpp:100] Creating Layer pool2
I1024 23:41:22.899708 12615 net.cpp:434] pool2 <- norm2
I1024 23:41:22.899719 12615 net.cpp:408] pool2 -> pool2
I1024 23:41:22.899775 12615 net.cpp:150] Setting up pool2
I1024 23:41:22.899781 12615 net.cpp:157] Top shape: 50 256 13 13 (2163200)
I1024 23:41:22.899785 12615 net.cpp:165] Memory required for data: 339782000
I1024 23:41:22.899790 12615 layer_factory.hpp:77] Creating layer conv3
I1024 23:41:22.899813 12615 net.cpp:100] Creating Layer conv3
I1024 23:41:22.899818 12615 net.cpp:434] conv3 <- pool2
I1024 23:41:22.899827 12615 net.cpp:408] conv3 -> conv3
I1024 23:41:22.935034 12615 net.cpp:150] Setting up conv3
I1024 23:41:22.935091 12615 net.cpp:157] Top shape: 50 384 13 13 (3244800)
I1024 23:41:22.935096 12615 net.cpp:165] Memory required for data: 352761200
I1024 23:41:22.935138 12615 layer_factory.hpp:77] Creating layer relu3
I1024 23:41:22.935161 12615 net.cpp:100] Creating Layer relu3
I1024 23:41:22.935183 12615 net.cpp:434] relu3 <- conv3
I1024 23:41:22.935194 12615 net.cpp:395] relu3 -> conv3 (in-place)
I1024 23:41:22.937191 12615 net.cpp:150] Setting up relu3
I1024 23:41:22.937230 12615 net.cpp:157] Top shape: 50 384 13 13 (3244800)
I1024 23:41:22.937235 12615 net.cpp:165] Memory required for data: 365740400
I1024 23:41:22.937242 12615 layer_factory.hpp:77] Creating layer conv4
I1024 23:41:22.937279 12615 net.cpp:100] Creating Layer conv4
I1024 23:41:22.937289 12615 net.cpp:434] conv4 <- conv3
I1024 23:41:22.937319 12615 net.cpp:408] conv4 -> conv4
I1024 23:41:22.969262 12615 net.cpp:150] Setting up conv4
I1024 23:41:22.969303 12615 net.cpp:157] Top shape: 50 384 13 13 (3244800)
I1024 23:41:22.969310 12615 net.cpp:165] Memory required for data: 378719600
I1024 23:41:22.969326 12615 layer_factory.hpp:77] Creating layer relu4
I1024 23:41:22.969358 12615 net.cpp:100] Creating Layer relu4
I1024 23:41:22.969367 12615 net.cpp:434] relu4 <- conv4
I1024 23:41:22.969390 12615 net.cpp:395] relu4 -> conv4 (in-place)
I1024 23:41:22.971411 12615 net.cpp:150] Setting up relu4
I1024 23:41:22.971441 12615 net.cpp:157] Top shape: 50 384 13 13 (3244800)
I1024 23:41:22.971446 12615 net.cpp:165] Memory required for data: 391698800
I1024 23:41:22.971463 12615 layer_factory.hpp:77] Creating layer conv5
I1024 23:41:22.971518 12615 net.cpp:100] Creating Layer conv5
I1024 23:41:22.971524 12615 net.cpp:434] conv5 <- conv4
I1024 23:41:22.971544 12615 net.cpp:408] conv5 -> conv5
I1024 23:41:22.996415 12615 net.cpp:150] Setting up conv5
I1024 23:41:22.996464 12615 net.cpp:157] Top shape: 50 256 13 13 (2163200)
I1024 23:41:22.996470 12615 net.cpp:165] Memory required for data: 400351600
I1024 23:41:22.996500 12615 layer_factory.hpp:77] Creating layer relu5
I1024 23:41:22.996520 12615 net.cpp:100] Creating Layer relu5
I1024 23:41:22.996533 12615 net.cpp:434] relu5 <- conv5
I1024 23:41:22.996551 12615 net.cpp:395] relu5 -> conv5 (in-place)
I1024 23:41:22.997323 12615 net.cpp:150] Setting up relu5
I1024 23:41:22.997339 12615 net.cpp:157] Top shape: 50 256 13 13 (2163200)
I1024 23:41:22.997342 12615 net.cpp:165] Memory required for data: 409004400
I1024 23:41:22.997347 12615 layer_factory.hpp:77] Creating layer pool5
I1024 23:41:22.997362 12615 net.cpp:100] Creating Layer pool5
I1024 23:41:22.997419 12615 net.cpp:434] pool5 <- conv5
I1024 23:41:22.997463 12615 net.cpp:408] pool5 -> pool5
I1024 23:41:22.997597 12615 net.cpp:150] Setting up pool5
I1024 23:41:22.997608 12615 net.cpp:157] Top shape: 50 256 6 6 (460800)
I1024 23:41:22.997614 12615 net.cpp:165] Memory required for data: 410847600
I1024 23:41:22.997620 12615 layer_factory.hpp:77] Creating layer fc6
I1024 23:41:22.997645 12615 net.cpp:100] Creating Layer fc6
I1024 23:41:22.997658 12615 net.cpp:434] fc6 <- pool5
I1024 23:41:22.997699 12615 net.cpp:408] fc6 -> fc6
I1024 23:41:24.528439 12615 net.cpp:150] Setting up fc6
I1024 23:41:24.528501 12615 net.cpp:157] Top shape: 50 4096 (204800)
I1024 23:41:24.528508 12615 net.cpp:165] Memory required for data: 411666800
I1024 23:41:24.528532 12615 layer_factory.hpp:77] Creating layer relu6
I1024 23:41:24.528553 12615 net.cpp:100] Creating Layer relu6
I1024 23:41:24.528564 12615 net.cpp:434] relu6 <- fc6
I1024 23:41:24.528576 12615 net.cpp:395] relu6 -> fc6 (in-place)
I1024 23:41:24.529336 12615 net.cpp:150] Setting up relu6
I1024 23:41:24.529362 12615 net.cpp:157] Top shape: 50 4096 (204800)
I1024 23:41:24.529367 12615 net.cpp:165] Memory required for data: 412486000
I1024 23:41:24.529371 12615 layer_factory.hpp:77] Creating layer drop6
I1024 23:41:24.529386 12615 net.cpp:100] Creating Layer drop6
I1024 23:41:24.529392 12615 net.cpp:434] drop6 <- fc6
I1024 23:41:24.529399 12615 net.cpp:395] drop6 -> fc6 (in-place)
I1024 23:41:24.529460 12615 net.cpp:150] Setting up drop6
I1024 23:41:24.529469 12615 net.cpp:157] Top shape: 50 4096 (204800)
I1024 23:41:24.529474 12615 net.cpp:165] Memory required for data: 413305200
I1024 23:41:24.529480 12615 layer_factory.hpp:77] Creating layer fc7
I1024 23:41:24.529497 12615 net.cpp:100] Creating Layer fc7
I1024 23:41:24.529501 12615 net.cpp:434] fc7 <- fc6
I1024 23:41:24.529511 12615 net.cpp:408] fc7 -> fc7
I1024 23:41:25.349483 12615 net.cpp:150] Setting up fc7
I1024 23:41:25.349529 12615 net.cpp:157] Top shape: 50 4096 (204800)
I1024 23:41:25.349534 12615 net.cpp:165] Memory required for data: 414124400
I1024 23:41:25.349556 12615 layer_factory.hpp:77] Creating layer relu7
I1024 23:41:25.349578 12615 net.cpp:100] Creating Layer relu7
I1024 23:41:25.349593 12615 net.cpp:434] relu7 <- fc7
I1024 23:41:25.349608 12615 net.cpp:395] relu7 -> fc7 (in-place)
I1024 23:41:25.350270 12615 net.cpp:150] Setting up relu7
I1024 23:41:25.350284 12615 net.cpp:157] Top shape: 50 4096 (204800)
I1024 23:41:25.350287 12615 net.cpp:165] Memory required for data: 414943600
I1024 23:41:25.350303 12615 layer_factory.hpp:77] Creating layer drop7
I1024 23:41:25.350319 12615 net.cpp:100] Creating Layer drop7
I1024 23:41:25.350327 12615 net.cpp:434] drop7 <- fc7
I1024 23:41:25.350370 12615 net.cpp:395] drop7 -> fc7 (in-place)
I1024 23:41:25.350430 12615 net.cpp:150] Setting up drop7
I1024 23:41:25.350438 12615 net.cpp:157] Top shape: 50 4096 (204800)
I1024 23:41:25.350441 12615 net.cpp:165] Memory required for data: 415762800
I1024 23:41:25.350445 12615 layer_factory.hpp:77] Creating layer fc8_2
I1024 23:41:25.350458 12615 net.cpp:100] Creating Layer fc8_2
I1024 23:41:25.350466 12615 net.cpp:434] fc8_2 <- fc7
I1024 23:41:25.350481 12615 net.cpp:408] fc8_2 -> fc8
I1024 23:41:25.350913 12615 net.cpp:150] Setting up fc8_2
I1024 23:41:25.350926 12615 net.cpp:157] Top shape: 50 2 (100)
I1024 23:41:25.350931 12615 net.cpp:165] Memory required for data: 415763200
I1024 23:41:25.350945 12615 layer_factory.hpp:77] Creating layer fc8_fc8_2_0_split
I1024 23:41:25.350975 12615 net.cpp:100] Creating Layer fc8_fc8_2_0_split
I1024 23:41:25.350986 12615 net.cpp:434] fc8_fc8_2_0_split <- fc8
I1024 23:41:25.350996 12615 net.cpp:408] fc8_fc8_2_0_split -> fc8_fc8_2_0_split_0
I1024 23:41:25.351024 12615 net.cpp:408] fc8_fc8_2_0_split -> fc8_fc8_2_0_split_1
I1024 23:41:25.351089 12615 net.cpp:150] Setting up fc8_fc8_2_0_split
I1024 23:41:25.351100 12615 net.cpp:157] Top shape: 50 2 (100)
I1024 23:41:25.351107 12615 net.cpp:157] Top shape: 50 2 (100)
I1024 23:41:25.351115 12615 net.cpp:165] Memory required for data: 415764000
I1024 23:41:25.351145 12615 layer_factory.hpp:77] Creating layer accuracy
I1024 23:41:25.351158 12615 net.cpp:100] Creating Layer accuracy
I1024 23:41:25.351183 12615 net.cpp:434] accuracy <- fc8_fc8_2_0_split_0
I1024 23:41:25.351191 12615 net.cpp:434] accuracy <- label_data_1_split_0
I1024 23:41:25.351200 12615 net.cpp:408] accuracy -> accuracy
I1024 23:41:25.351214 12615 net.cpp:150] Setting up accuracy
I1024 23:41:25.351222 12615 net.cpp:157] Top shape: (1)
I1024 23:41:25.351230 12615 net.cpp:165] Memory required for data: 415764004
I1024 23:41:25.351233 12615 layer_factory.hpp:77] Creating layer loss
I1024 23:41:25.351240 12615 net.cpp:100] Creating Layer loss
I1024 23:41:25.351245 12615 net.cpp:434] loss <- fc8_fc8_2_0_split_1
I1024 23:41:25.351249 12615 net.cpp:434] loss <- label_data_1_split_1
I1024 23:41:25.351254 12615 net.cpp:408] loss -> loss
I1024 23:41:25.351263 12615 layer_factory.hpp:77] Creating layer loss
I1024 23:41:25.351945 12615 net.cpp:150] Setting up loss
I1024 23:41:25.351963 12615 net.cpp:157] Top shape: (1)
I1024 23:41:25.351968 12615 net.cpp:160]     with loss weight 1
I1024 23:41:25.351979 12615 net.cpp:165] Memory required for data: 415764008
I1024 23:41:25.351984 12615 net.cpp:226] loss needs backward computation.
I1024 23:41:25.351989 12615 net.cpp:228] accuracy does not need backward computation.
I1024 23:41:25.351992 12615 net.cpp:226] fc8_fc8_2_0_split needs backward computation.
I1024 23:41:25.351995 12615 net.cpp:226] fc8_2 needs backward computation.
I1024 23:41:25.351999 12615 net.cpp:226] drop7 needs backward computation.
I1024 23:41:25.352002 12615 net.cpp:226] relu7 needs backward computation.
I1024 23:41:25.352005 12615 net.cpp:226] fc7 needs backward computation.
I1024 23:41:25.352008 12615 net.cpp:226] drop6 needs backward computation.
I1024 23:41:25.352011 12615 net.cpp:226] relu6 needs backward computation.
I1024 23:41:25.352015 12615 net.cpp:226] fc6 needs backward computation.
I1024 23:41:25.352021 12615 net.cpp:226] pool5 needs backward computation.
I1024 23:41:25.352025 12615 net.cpp:226] relu5 needs backward computation.
I1024 23:41:25.352030 12615 net.cpp:226] conv5 needs backward computation.
I1024 23:41:25.352033 12615 net.cpp:226] relu4 needs backward computation.
I1024 23:41:25.352036 12615 net.cpp:226] conv4 needs backward computation.
I1024 23:41:25.352051 12615 net.cpp:226] relu3 needs backward computation.
I1024 23:41:25.352056 12615 net.cpp:226] conv3 needs backward computation.
I1024 23:41:25.352061 12615 net.cpp:226] pool2 needs backward computation.
I1024 23:41:25.352067 12615 net.cpp:226] norm2 needs backward computation.
I1024 23:41:25.352071 12615 net.cpp:226] relu2 needs backward computation.
I1024 23:41:25.352073 12615 net.cpp:226] conv2 needs backward computation.
I1024 23:41:25.352077 12615 net.cpp:226] pool1 needs backward computation.
I1024 23:41:25.352080 12615 net.cpp:226] norm1 needs backward computation.
I1024 23:41:25.352083 12615 net.cpp:226] relu1 needs backward computation.
I1024 23:41:25.352087 12615 net.cpp:226] conv1 needs backward computation.
I1024 23:41:25.352092 12615 net.cpp:228] label_data_1_split does not need backward computation.
I1024 23:41:25.352097 12615 net.cpp:228] data does not need backward computation.
I1024 23:41:25.352100 12615 net.cpp:270] This network produces output accuracy
I1024 23:41:25.352105 12615 net.cpp:270] This network produces output loss
I1024 23:41:25.352125 12615 net.cpp:283] Network initialization done.
I1024 23:41:25.352293 12615 solver.cpp:60] Solver scaffolding done.
I1024 23:41:25.352947 12615 caffe.cpp:155] Finetuning from ./alexnet/bvlc_alexnet.caffemodel
I1024 23:41:28.064100 12615 upgrade_proto.cpp:43] Attempting to upgrade input file specified using deprecated transformation parameters: ./alexnet/bvlc_alexnet.caffemodel
I1024 23:41:28.064154 12615 upgrade_proto.cpp:46] Successfully upgraded file specified using deprecated data transformation parameters.
W1024 23:41:28.064159 12615 upgrade_proto.cpp:48] Note that future Caffe releases will only support transform_param messages for transformation fields.
I1024 23:41:28.064308 12615 upgrade_proto.cpp:52] Attempting to upgrade input file specified using deprecated V1LayerParameter: ./alexnet/bvlc_alexnet.caffemodel
I1024 23:41:30.182667 12615 upgrade_proto.cpp:60] Successfully upgraded file specified using deprecated V1LayerParameter
I1024 23:41:30.248891 12615 net.cpp:761] Ignoring source layer fc8
I1024 23:41:32.553134 12615 upgrade_proto.cpp:43] Attempting to upgrade input file specified using deprecated transformation parameters: ./alexnet/bvlc_alexnet.caffemodel
I1024 23:41:32.553164 12615 upgrade_proto.cpp:46] Successfully upgraded file specified using deprecated data transformation parameters.
W1024 23:41:32.553169 12615 upgrade_proto.cpp:48] Note that future Caffe releases will only support transform_param messages for transformation fields.
I1024 23:41:32.553191 12615 upgrade_proto.cpp:52] Attempting to upgrade input file specified using deprecated V1LayerParameter: ./alexnet/bvlc_alexnet.caffemodel
I1024 23:41:34.326241 12615 upgrade_proto.cpp:60] Successfully upgraded file specified using deprecated V1LayerParameter
I1024 23:41:34.404848 12615 net.cpp:761] Ignoring source layer fc8
I1024 23:41:34.431084 12615 caffe.cpp:251] Starting Optimization
I1024 23:41:34.431143 12615 solver.cpp:279] Solving AlexNet
I1024 23:41:34.431152 12615 solver.cpp:280] Learning Rate Policy: step
I1024 23:41:35.018268 12615 solver.cpp:228] Iteration 0, loss = 0.818738
I1024 23:41:35.018312 12615 solver.cpp:244]     Train net output #0: loss = 0.818738 (* 1 = 0.818738 loss)
I1024 23:41:35.018332 12615 sgd_solver.cpp:106] Iteration 0, lr = 0.001
